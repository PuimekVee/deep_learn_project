{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# LSTM Sentiment Classifier (TensorFlow/Keras)\n",
        "# ===============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# --- 0. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸à¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ ---\n",
        "# à¹ƒà¸Šà¹‰à¸„à¹ˆà¸²à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¹ƒà¸™à¹‚à¸¡à¹€à¸”à¸¥ PyTorch à¸‚à¸­à¸‡à¸„à¸¸à¸“\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT = 0.4\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "# à¸à¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸š Keras TextVectorization\n",
        "MAX_SEQUENCE_LENGTH = 250  # à¸à¸³à¸«à¸™à¸”à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸›à¸£à¸°à¹‚à¸¢à¸„ (PyTorch à¸—à¸³ padding à¸•à¸²à¸¡ batch)\n",
        "MAX_TOKENS = 20000         # à¸à¸³à¸«à¸™à¸”à¸‚à¸™à¸²à¸”à¸‚à¸­à¸‡à¸„à¸³à¸¨à¸±à¸à¸—à¹Œ (Vocab size)\n",
        "\n",
        "# --- 1. à¹‚à¸«à¸¥à¸” Dataset ---\n",
        "# ! à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸: à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ \"amazon_reviews.csv\" à¹„à¸›à¸¢à¸±à¸‡ Colab à¸à¹ˆà¸­à¸™\n",
        "try:\n",
        "    df = pd.read_csv(\"amazon_reviews.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸš¨ à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ 'amazon_reviews.csv'\")\n",
        "    print(\"à¸à¸£à¸¸à¸“à¸²à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¹„à¸›à¸¢à¸±à¸‡ Colab (à¸¥à¸²à¸à¹„à¸Ÿà¸¥à¹Œà¸§à¸²à¸‡à¹ƒà¸™à¹à¸–à¸š 'Files' à¸”à¹‰à¸²à¸™à¸‹à¹‰à¸²à¸¢) à¹à¸¥à¹‰à¸§à¸£à¸±à¸™à¹€à¸‹à¸¥à¸¥à¹Œà¸™à¸µà¹‰à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡\")\n",
        "    print(\"=\"*50)\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡ DataFrame à¹€à¸›à¸¥à¹ˆà¸²à¹€à¸à¸·à¹ˆà¸­à¸à¸±à¸™à¹‚à¸„à¹‰à¸”à¹à¸„à¸£à¸Šà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸£à¸­à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”\n",
        "    df = pd.DataFrame(columns=[\"reviewText\", \"overall\"])\n",
        "\n",
        "if not df.empty:\n",
        "    # keep only useful columns\n",
        "    df = df[[\"reviewText\", \"overall\"]].dropna()\n",
        "\n",
        "    # convert ratings to binary sentiment\n",
        "    df = df[df[\"overall\"] != 3]  # remove neutral\n",
        "    df[\"label\"] = (df[\"overall\"] >= 4).astype(int)\n",
        "\n",
        "    # --- 2. Upsample minority class (negative = 0) ---\n",
        "    # à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£ à¹€à¸à¸£à¸²à¸°à¹ƒà¸Šà¹‰ pandas à¹à¸¥à¸° sklearn\n",
        "    pos_df = df[df[\"label\"] == 1]\n",
        "    neg_df = df[df[\"label\"] == 0]\n",
        "\n",
        "    # upsample the negatives to match positives\n",
        "    neg_df_upsampled = resample(\n",
        "        neg_df,\n",
        "        replace=True,         # sample with replacement\n",
        "        n_samples=len(pos_df),  # match number of positive samples\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # combine and shuffle\n",
        "    df_balanced = pd.concat([pos_df, neg_df_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(\"Before upsampling:\")\n",
        "    print(df[\"label\"].value_counts())\n",
        "    print(\"\\nAfter upsampling:\")\n",
        "    print(df_balanced[\"label\"].value_counts())\n",
        "\n",
        "    # --- 3. Split data ---\n",
        "    # à¹à¸¢à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™ (Keras à¸ˆà¸° .adapt() à¹€à¸‰à¸à¸²à¸°à¸šà¸™ train set à¹€à¸à¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ data leakage)\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "        df_balanced[\"reviewText\"], df_balanced[\"label\"].values, # .values à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ numpy array\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # --- 4. Tokenizer à¹à¸¥à¸° Encoding (Keras way) ---\n",
        "    # Keras à¹ƒà¸Šà¹‰ TextVectorization layer à¹ƒà¸™à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ Tokenize, Build Vocab, à¹à¸¥à¸° Encode\n",
        "    # à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸™à¸µà¹‰à¹ƒà¸™à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¹€à¸”à¸µà¸¢à¸§\n",
        "\n",
        "    vectorize_layer = layers.TextVectorization(\n",
        "        max_tokens=MAX_TOKENS,             # à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸²à¸à¸±à¸š vocab_size\n",
        "        output_mode='int',                 # à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸²à¸à¸±à¸š encode() à¸‚à¸­à¸‡à¸„à¸¸à¸“\n",
        "        output_sequence_length=MAX_SEQUENCE_LENGTH # à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸²à¸à¸±à¸š pad_sequence\n",
        "    )\n",
        "\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡ Vocabulary (à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸²à¸à¸±à¸šà¸à¸²à¸£à¸§à¸™à¸¥à¸¹à¸›à¸ªà¸£à¹‰à¸²à¸‡ vocab dict)\n",
        "    # .adapt() à¸ˆà¸°à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ training\n",
        "    print(\"\\nAdapting TextVectorization layer...\")\n",
        "    vectorize_layer.adapt(train_texts)\n",
        "\n",
        "    # à¸”à¸¹à¸‚à¸™à¸²à¸” Vocab à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¹„à¸”à¹‰ (à¸£à¸§à¸¡ <PAD> à¹à¸¥à¸° <UNK>)\n",
        "    VOCAB_SIZE = vectorize_layer.vocabulary_size()\n",
        "    print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "\n",
        "    # --- 5. à¸ªà¸£à¹‰à¸²à¸‡ tf.data.Dataset (Keras/TF way) ---\n",
        "    # à¸™à¸µà¹ˆà¸„à¸·à¸­à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸¡à¸²à¹à¸—à¸™ Dataset à¹à¸¥à¸° DataLoader à¸‚à¸­à¸‡ PyTorch\n",
        "\n",
        "    def vectorize_text(text, label):\n",
        "        text = tf.expand_dims(text, -1) # à¸‚à¸¢à¸²à¸¢à¸¡à¸´à¸•à¸´à¸ªà¸³à¸«à¸£à¸±à¸š vectorizer\n",
        "        return vectorize_layer(text), label\n",
        "\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡ Pipelined Datasets\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
        "    train_ds = train_ds.batch(BATCH_SIZE).map(vectorize_text).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))\n",
        "    test_ds = test_ds.batch(BATCH_SIZE).map(vectorize_text).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # --- 6. Model (Keras way) ---\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡à¸à¸±à¸šà¸ªà¸–à¸²à¸›à¸±à¸•à¸¢à¸à¸£à¸£à¸¡ PyTorch à¸‚à¸­à¸‡à¸„à¸¸à¸“\n",
        "\n",
        "    # mask_zero=True à¸ˆà¸°à¸šà¸­à¸à¹ƒà¸«à¹‰ LSTM layer à¹„à¸¡à¹ˆà¸ªà¸™à¹ƒà¸ˆ padding (à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸² padding_idx=0)\n",
        "    embedding_layer = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        # Input layer (à¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸š TextVectorization à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¸™à¸­à¸à¹‚à¸¡à¹€à¸”à¸¥)\n",
        "        layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int64\"),\n",
        "\n",
        "        # 1. Embedding\n",
        "        embedding_layer,\n",
        "\n",
        "        # 2. Dropout à¸«à¸¥à¸±à¸‡ Embedding (à¹€à¸«à¸¡à¸·à¸­à¸™à¹ƒà¸™ PyTorch)\n",
        "        layers.Dropout(DROPOUT),\n",
        "\n",
        "        # 3. LSTM Layer 1 (Bidirectional)\n",
        "        # PyTorch: num_layers=2, dropout=0.4 (à¸„à¸·à¸­ dropout *à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡* layer)\n",
        "        # Keras: à¸•à¹‰à¸­à¸‡ stack 2 layer à¹à¸¥à¸°à¹ƒà¸ªà¹ˆ Dropout layer à¸„à¸±à¹ˆà¸™à¸à¸¥à¸²à¸‡\n",
        "        layers.Bidirectional(\n",
        "            layers.LSTM(HIDDEN_DIM, return_sequences=True) # return_sequences=True à¹€à¸à¸·à¹ˆà¸­à¸ªà¹ˆà¸‡à¸•à¹ˆà¸­à¹ƒà¸«à¹‰ layer à¸–à¸±à¸”à¹„à¸›\n",
        "        ),\n",
        "\n",
        "        # Dropout *à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡* LSTM layers (à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸² dropout=0.4 à¹ƒà¸™ nn.LSTM)\n",
        "        layers.Dropout(DROPOUT),\n",
        "\n",
        "        # 4. LSTM Layer 2 (Bidirectional)\n",
        "        layers.Bidirectional(\n",
        "            layers.LSTM(HIDDEN_DIM, return_sequences=False) # False à¸„à¸·à¸­à¹€à¸­à¸²à¹à¸„à¹ˆ state à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢\n",
        "        ),\n",
        "\n",
        "        # 5. Dropout à¸šà¸™ Hiddent State à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹ƒà¸™ PyTorch)\n",
        "        layers.Dropout(DROPOUT),\n",
        "\n",
        "        # 6. Fully Connected (Dense) Layer\n",
        "        # Keras à¸ˆà¸°à¸£à¸§à¸¡ Sigmoid activation à¹„à¸§à¹‰à¹ƒà¸™ Layer à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢\n",
        "        # PyTorch: nn.Linear(hidden_dim * 2, 1) + nn.Sigmoid()\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # --- 7. Training setup (Keras way) ---\n",
        "    # .compile() à¸ˆà¸°à¸£à¸§à¸šà¸£à¸§à¸¡ Optimizer, Loss, à¹à¸¥à¸° Metrics\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy', # à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸² nn.BCELoss\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            learning_rate=0.001,\n",
        "            clipnorm=5.0 # à¹€à¸—à¸µà¸¢à¸šà¹€à¸—à¹ˆà¸² clip_grad_norm_\n",
        "        ),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # --- 8. Train (Keras way) ---\n",
        "    # model.fit() à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸²à¸£ training loop à¹à¸¥à¸° validation loop à¹ƒà¸«à¹‰à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”\n",
        "    print(\"\\n--- ğŸš€ Starting Training ---\")\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_ds # à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ test set à¹„à¸›à¸”à¹‰à¸§à¸¢à¹€à¸¥à¸¢à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° epoch\n",
        "    )\n",
        "\n",
        "    # --- 9. Evaluate (Keras way) ---\n",
        "    # à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰ model.evaluate() à¸«à¸£à¸·à¸­à¸”à¸¹à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ˆà¸²à¸ .fit()\n",
        "    print(\"\\n--- ğŸ“Š Evaluating Model ---\")\n",
        "    loss, accuracy = model.evaluate(test_ds)\n",
        "    print(f\"\\nâœ… Test Accuracy (Balanced via Upsampling): {accuracy:.4f}\")\n",
        "\n",
        "    # --- 10. Evaluate with metrics (Keras way) ---\n",
        "    # à¹ƒà¸Šà¹‰ model.predict() à¹€à¸à¸·à¹ˆà¸­à¹€à¸à¹‡à¸š predictions à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”\n",
        "    all_preds_prob = model.predict(test_ds)\n",
        "    all_preds = (all_preds_prob > 0.5).astype(int).squeeze()\n",
        "\n",
        "    # à¹€à¸£à¸²à¸•à¹‰à¸­à¸‡à¸”à¸¶à¸‡ labels à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸ˆà¸²à¸ test_ds à¸”à¹‰à¸§à¸¢\n",
        "    all_labels = np.concatenate([labels for _, labels in test_ds], axis=0)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nğŸ“Š Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"], digits=4))\n",
        "\n",
        "    # Optional: show confusion matrix\n",
        "    print(\"\\nğŸ§¾ Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "    # --- 11. Predict new sentences ---\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ predict à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰ vectorize_layer à¸—à¸µà¹ˆà¹€à¸£à¸² adapt à¹„à¸§à¹‰\n",
        "    def predict_sentiment(text):\n",
        "        # Keras model à¸„à¸²à¸”à¸«à¸§à¸±à¸‡ input à¹€à¸›à¹‡à¸™ batch\n",
        "        # 1. à¸«à¹ˆà¸­ text à¸”à¹‰à¸§à¸¢ list -> [\"...text...\"]\n",
        "        # 2. à¸ªà¹ˆà¸‡à¹€à¸‚à¹‰à¸² vectorize_layer -> à¹„à¸”à¹‰ Tensor à¸£à¸¹à¸›à¸£à¹ˆà¸²à¸‡ (1, MAX_SEQUENCE_LENGTH)\n",
        "        encoded_text = vectorize_layer([text])\n",
        "\n",
        "        # 3. à¸ªà¹ˆà¸‡à¹€à¸‚à¹‰à¸² model.predict\n",
        "        prediction = model.predict(encoded_text, verbose=0) # verbose=0 à¹€à¸à¸·à¹ˆà¸­à¸›à¸´à¸” loading bar\n",
        "        pred_prob = prediction[0][0] # à¹„à¸”à¹‰à¸„à¹ˆà¸² probability (à¹€à¸Šà¹ˆà¸™ 0.98)\n",
        "\n",
        "        return \"Positive\" if pred_prob > 0.5 else \"Negative\"\n",
        "\n",
        "    # --- 12. Try multiple examples ---\n",
        "    positive_texts = [\n",
        "        \"I really love this product!\",\n",
        "        \"Absolutely fantastic, exceeded my expectations.\",\n",
        "        \"This is the best purchase I've made this year.\",\n",
        "        \"High quality and works perfectly.\",\n",
        "        \"I am very satisfied with this item.\",\n",
        "        \"Excellent product, would buy again.\",\n",
        "        \"Totally worth the money, highly recommend.\",\n",
        "        \"Amazing! Fast shipping and great quality.\",\n",
        "        \"Very happy with this, five stars!\",\n",
        "        \"This product is incredible, love it!\"\n",
        "    ]\n",
        "\n",
        "    negative_texts = [\n",
        "        \"Terrible quality, very disappointed.\",\n",
        "        \"Broke after one use, waste of money.\",\n",
        "        \"Do not buy this, completely useless.\",\n",
        "        \"Poorly made and arrived late.\",\n",
        "        \"Not as described, extremely unhappy.\",\n",
        "        \"This product is awful, avoid it.\",\n",
        "        \"Very disappointed, will never buy again.\",\n",
        "        \"Cheap material, doesn't work at all.\",\n",
        "        \"Horrible experience, one star.\",\n",
        "        \"Regret buying this, total waste.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Positive Sentiment Predictions ---\")\n",
        "    for text in positive_texts:\n",
        "        sentiment = predict_sentiment(text)\n",
        "        print(f\"'{text}' -> {sentiment}\")\n",
        "\n",
        "    print(\"\\n--- Negative Sentiment Predictions ---\")\n",
        "    for text in negative_texts:\n",
        "        sentiment = predict_sentiment(text)\n",
        "        print(f\"'{text}' -> {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s5n6Em2bkTL8",
        "outputId": "8b9c959a-0f4f-4894-82ef-809bbf024377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before upsampling:\n",
            "label\n",
            "1    4448\n",
            "0     324\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After upsampling:\n",
            "label\n",
            "0    4448\n",
            "1    4448\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Adapting TextVectorization layer...\n",
            "Vocabulary size: 9719\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚     \u001b[38;5;34m2,488,064\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚     \u001b[38;5;34m3,149,824\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           â”‚     \u001b[38;5;34m6,295,552\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚         \u001b[38;5;34m1,025\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,488,064</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,149,824</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,295,552</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,934,465\u001b[0m (45.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,934,465</span> (45.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,934,465\u001b[0m (45.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,934,465</span> (45.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ğŸš€ Starting Training ---\n",
            "Epoch 1/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 173ms/step - accuracy: 0.8020 - loss: 0.4682 - val_accuracy: 0.9624 - val_loss: 0.1150\n",
            "Epoch 2/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.9707 - loss: 0.0904 - val_accuracy: 0.9685 - val_loss: 0.1166\n",
            "Epoch 3/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 173ms/step - accuracy: 0.9625 - loss: 0.1303 - val_accuracy: 0.9820 - val_loss: 0.0943\n",
            "Epoch 4/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 170ms/step - accuracy: 0.9924 - loss: 0.0410 - val_accuracy: 0.9792 - val_loss: 0.0555\n",
            "Epoch 5/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 169ms/step - accuracy: 0.9930 - loss: 0.0286 - val_accuracy: 0.9871 - val_loss: 0.0364\n",
            "Epoch 6/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 169ms/step - accuracy: 0.9953 - loss: 0.0194 - val_accuracy: 0.9893 - val_loss: 0.0319\n",
            "Epoch 7/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 171ms/step - accuracy: 0.9979 - loss: 0.0084 - val_accuracy: 0.9803 - val_loss: 0.0870\n",
            "Epoch 8/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.9875 - loss: 0.0463 - val_accuracy: 0.9669 - val_loss: 0.1115\n",
            "Epoch 9/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 170ms/step - accuracy: 0.9955 - loss: 0.0156 - val_accuracy: 0.9871 - val_loss: 0.0452\n",
            "Epoch 10/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 171ms/step - accuracy: 0.9989 - loss: 0.0067 - val_accuracy: 0.9933 - val_loss: 0.0350\n",
            "\n",
            "--- ğŸ“Š Evaluating Model ---\n",
            "\u001b[1m28/28\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9924 - loss: 0.0391\n",
            "\n",
            "âœ… Test Accuracy (Balanced via Upsampling): 0.9933\n",
            "\u001b[1m28/28\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step\n",
            "\n",
            "ğŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.9865    1.0000    0.9932       878\n",
            "    Positive     1.0000    0.9867    0.9933       902\n",
            "\n",
            "    accuracy                         0.9933      1780\n",
            "   macro avg     0.9933    0.9933    0.9933      1780\n",
            "weighted avg     0.9933    0.9933    0.9933      1780\n",
            "\n",
            "\n",
            "ğŸ§¾ Confusion Matrix:\n",
            "[[878   0]\n",
            " [ 12 890]]\n",
            "\n",
            "--- Positive Sentiment Predictions ---\n",
            "'I really love this product!' -> Positive\n",
            "'Absolutely fantastic, exceeded my expectations.' -> Positive\n",
            "'This is the best purchase I've made this year.' -> Positive\n",
            "'High quality and works perfectly.' -> Positive\n",
            "'I am very satisfied with this item.' -> Positive\n",
            "'Excellent product, would buy again.' -> Positive\n",
            "'Totally worth the money, highly recommend.' -> Positive\n",
            "'Amazing! Fast shipping and great quality.' -> Positive\n",
            "'Very happy with this, five stars!' -> Positive\n",
            "'This product is incredible, love it!' -> Positive\n",
            "\n",
            "--- Negative Sentiment Predictions ---\n",
            "'Terrible quality, very disappointed.' -> Negative\n",
            "'Broke after one use, waste of money.' -> Negative\n",
            "'Do not buy this, completely useless.' -> Negative\n",
            "'Poorly made and arrived late.' -> Positive\n",
            "'Not as described, extremely unhappy.' -> Positive\n",
            "'This product is awful, avoid it.' -> Negative\n",
            "'Very disappointed, will never buy again.' -> Negative\n",
            "'Cheap material, doesn't work at all.' -> Positive\n",
            "'Horrible experience, one star.' -> Positive\n",
            "'Regret buying this, total waste.' -> Negative\n"
          ]
        }
      ]
    }
  ]
}